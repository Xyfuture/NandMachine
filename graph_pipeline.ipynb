{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ecf6fe35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config loaded successfully:\n",
      "Hidden size: 4096\n",
      "Num attention heads: 32\n",
      "Num layers: 36\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import json\n",
    "import torch\n",
    "from transformers import Qwen3Config\n",
    "from nandmachine.frontend.network.qwen3 import Qwen3DecoderLayer\n",
    "from nandmachine.frontend.core.graph.base import NxTracer\n",
    "from nandmachine.frontend.network.torch_kernels import *\n",
    "\n",
    "# Load config from JSON\n",
    "with open('model_cards/qwen3-8B.json', 'r') as f:\n",
    "    config_dict = json.load(f)\n",
    "\n",
    "config = Qwen3Config(**config_dict)\n",
    "print(\"Config loaded successfully:\")\n",
    "print(f\"Hidden size: {config.hidden_size}\")\n",
    "print(f\"Num attention heads: {config.num_attention_heads}\")\n",
    "print(f\"Num layers: {config.num_hidden_layers}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02b58096",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch._subclasses.fake_tensor import FakeTensorMode\n",
    "from torch.fx.passes.fake_tensor_prop import FakeTensorProp\n",
    "\n",
    "fake_mode = FakeTensorMode(allow_non_fake_inputs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed905ff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Qwen3DecoderLayer created successfully\n",
      "\n",
      "Tracing the computation graph...\n",
      "Graph traced successfully!\n",
      "Total nodes in graph: 57\n"
     ]
    }
   ],
   "source": [
    "# Create Qwen3DecoderLayer instance\n",
    "with fake_mode:\n",
    "    layer = Qwen3DecoderLayer(config)\n",
    "    layer.eval()\n",
    "print(\"\\nQwen3DecoderLayer created successfully\")\n",
    "\n",
    "# Trace the computation graph using NxTracer\n",
    "tracer = NxTracer()\n",
    "\n",
    "print(\"\\nTracing the computation graph...\")\n",
    "graph = tracer.trace(layer)\n",
    "gm = torch.fx.GraphModule(layer,graph)\n",
    "print(\"Graph traced successfully!\")\n",
    "print(f\"Total nodes in graph: {len(graph.nodes)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5661adea",
   "metadata": {},
   "outputs": [],
   "source": [
    "with fake_mode:\n",
    "    input_hidden_states = torch.empty([16,1,4096])\n",
    "    input_position = torch.empty([16],dtype=torch.int)\n",
    "\n",
    "\n",
    "fake_prop = FakeTensorProp(gm,mode=fake_mode)\n",
    "\n",
    "fake_prop.propagate(input_position,input_hidden_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "334e1052",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nandmachine.frontend.core.passes.recorder import RecorderPass\n",
    "\n",
    "recorder_pass = RecorderPass()\n",
    "\n",
    "graph = recorder_pass.transform(gm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "264c328d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node name:positions meta: {'val': FakeTensor(..., size=(16,), dtype=torch.int32), 'output_shapes': (16,)}\n",
      "node name:hidden_states meta: {'val': FakeTensor(..., size=(16, 1, 4096)), 'output_shapes': (16, 1, 4096)}\n",
      "node name:self_attn_qkv_proj_weight meta: {'nn_module_stack': OrderedDict({'self_attn': ('self_attn', <class 'nandmachine.frontend.network.qwen3.Qwen3Attention'>), 'self_attn.qkv_proj': ('self_attn.qkv_proj', <class 'nandmachine.frontend.network.torch_kernels.QKVParallelLinear'>)}), 'val': FakeTensor(..., size=(6144, 4096)), 'output_shapes': (6144, 4096)}\n",
      "node name:linear meta: {'nn_module_stack': OrderedDict({'self_attn': ('self_attn', <class 'nandmachine.frontend.network.qwen3.Qwen3Attention'>), 'self_attn.qkv_proj': ('self_attn.qkv_proj', <class 'nandmachine.frontend.network.torch_kernels.QKVParallelLinear'>)}), 'val': FakeTensor(..., size=(16, 1, 6144)), 'output_shapes': (16, 1, 6144), 'input_shapes': [(16, 1, 4096), (6144, 4096)]}\n",
      "node name:split meta: {'nn_module_stack': OrderedDict({'self_attn': ('self_attn', <class 'nandmachine.frontend.network.qwen3.Qwen3Attention'>)}), 'val': (FakeTensor(..., size=(16, 1, 4096)), FakeTensor(..., size=(16, 1, 1024)), FakeTensor(..., size=(16, 1, 1024))), 'output_shapes': [(16, 1, 4096), (16, 1, 1024), (16, 1, 1024)], 'input_shapes': [(16, 1, 6144)]}\n",
      "node name:getitem meta: {'nn_module_stack': OrderedDict({'self_attn': ('self_attn', <class 'nandmachine.frontend.network.qwen3.Qwen3Attention'>)}), 'val': FakeTensor(..., size=(16, 1, 4096)), 'output_shapes': (16, 1, 4096), 'input_shapes': [[(16, 1, 4096), (16, 1, 1024), (16, 1, 1024)]]}\n",
      "node name:getitem_1 meta: {'nn_module_stack': OrderedDict({'self_attn': ('self_attn', <class 'nandmachine.frontend.network.qwen3.Qwen3Attention'>)}), 'val': FakeTensor(..., size=(16, 1, 1024)), 'output_shapes': (16, 1, 1024), 'input_shapes': [[(16, 1, 4096), (16, 1, 1024), (16, 1, 1024)]]}\n",
      "node name:getitem_2 meta: {'nn_module_stack': OrderedDict({'self_attn': ('self_attn', <class 'nandmachine.frontend.network.qwen3.Qwen3Attention'>)}), 'val': FakeTensor(..., size=(16, 1, 1024)), 'output_shapes': (16, 1, 1024), 'input_shapes': [[(16, 1, 4096), (16, 1, 1024), (16, 1, 1024)]]}\n",
      "node name:view meta: {'nn_module_stack': OrderedDict({'self_attn': ('self_attn', <class 'nandmachine.frontend.network.qwen3.Qwen3Attention'>)}), 'val': FakeTensor(..., size=(16, 32, 128)), 'output_shapes': (16, 32, 128), 'input_shapes': [(16, 1, 4096)]}\n",
      "node name:view_1 meta: {'nn_module_stack': OrderedDict({'self_attn': ('self_attn', <class 'nandmachine.frontend.network.qwen3.Qwen3Attention'>)}), 'val': FakeTensor(..., size=(16, 8, 128)), 'output_shapes': (16, 8, 128), 'input_shapes': [(16, 1, 1024)]}\n",
      "node name:view_2 meta: {'nn_module_stack': OrderedDict({'self_attn': ('self_attn', <class 'nandmachine.frontend.network.qwen3.Qwen3Attention'>)}), 'val': FakeTensor(..., size=(16, 8, 128)), 'output_shapes': (16, 8, 128), 'input_shapes': [(16, 1, 1024)]}\n",
      "node name:self_attn_rotary_emb_cos_sin_cache meta: {'nn_module_stack': OrderedDict({'self_attn': ('self_attn', <class 'nandmachine.frontend.network.qwen3.Qwen3Attention'>), 'self_attn.rotary_emb': ('self_attn.rotary_emb', <class 'nandmachine.frontend.network.torch_kernels.RotaryEmbedding'>)}), 'val': FakeTensor(..., size=(40960, 1, 128)), 'output_shapes': (40960, 1, 128)}\n",
      "node name:getitem_3 meta: {'nn_module_stack': OrderedDict({'self_attn': ('self_attn', <class 'nandmachine.frontend.network.qwen3.Qwen3Attention'>), 'self_attn.rotary_emb': ('self_attn.rotary_emb', <class 'nandmachine.frontend.network.torch_kernels.RotaryEmbedding'>)}), 'val': FakeTensor(..., size=(16, 1, 128)), 'output_shapes': (16, 1, 128), 'input_shapes': [(40960, 1, 128), (16,)]}\n",
      "node name:chunk meta: {'nn_module_stack': OrderedDict({'self_attn': ('self_attn', <class 'nandmachine.frontend.network.qwen3.Qwen3Attention'>), 'self_attn.rotary_emb': ('self_attn.rotary_emb', <class 'nandmachine.frontend.network.torch_kernels.RotaryEmbedding'>)}), 'val': (FakeTensor(..., size=(16, 1, 64)), FakeTensor(..., size=(16, 1, 64))), 'output_shapes': [(16, 1, 64), (16, 1, 64)], 'input_shapes': [(16, 1, 128)]}\n",
      "node name:getitem_4 meta: {'nn_module_stack': OrderedDict({'self_attn': ('self_attn', <class 'nandmachine.frontend.network.qwen3.Qwen3Attention'>), 'self_attn.rotary_emb': ('self_attn.rotary_emb', <class 'nandmachine.frontend.network.torch_kernels.RotaryEmbedding'>)}), 'val': FakeTensor(..., size=(16, 1, 64)), 'output_shapes': (16, 1, 64), 'input_shapes': [[(16, 1, 64), (16, 1, 64)]]}\n",
      "node name:getitem_5 meta: {'nn_module_stack': OrderedDict({'self_attn': ('self_attn', <class 'nandmachine.frontend.network.qwen3.Qwen3Attention'>), 'self_attn.rotary_emb': ('self_attn.rotary_emb', <class 'nandmachine.frontend.network.torch_kernels.RotaryEmbedding'>)}), 'val': FakeTensor(..., size=(16, 1, 64)), 'output_shapes': (16, 1, 64), 'input_shapes': [[(16, 1, 64), (16, 1, 64)]]}\n",
      "node name:float_1 meta: {'nn_module_stack': OrderedDict({'self_attn': ('self_attn', <class 'nandmachine.frontend.network.qwen3.Qwen3Attention'>), 'self_attn.rotary_emb': ('self_attn.rotary_emb', <class 'nandmachine.frontend.network.torch_kernels.RotaryEmbedding'>)}), 'val': FakeTensor(..., size=(16, 32, 128)), 'output_shapes': (16, 32, 128), 'input_shapes': [(16, 32, 128)]}\n",
      "node name:chunk_1 meta: {'nn_module_stack': OrderedDict({'self_attn': ('self_attn', <class 'nandmachine.frontend.network.qwen3.Qwen3Attention'>), 'self_attn.rotary_emb': ('self_attn.rotary_emb', <class 'nandmachine.frontend.network.torch_kernels.RotaryEmbedding'>)}), 'val': (FakeTensor(..., size=(16, 32, 64)), FakeTensor(..., size=(16, 32, 64))), 'output_shapes': [(16, 32, 64), (16, 32, 64)], 'input_shapes': [(16, 32, 128)]}\n",
      "node name:getitem_6 meta: {'nn_module_stack': OrderedDict({'self_attn': ('self_attn', <class 'nandmachine.frontend.network.qwen3.Qwen3Attention'>), 'self_attn.rotary_emb': ('self_attn.rotary_emb', <class 'nandmachine.frontend.network.torch_kernels.RotaryEmbedding'>)}), 'val': FakeTensor(..., size=(16, 32, 64)), 'output_shapes': (16, 32, 64), 'input_shapes': [[(16, 32, 64), (16, 32, 64)]]}\n",
      "node name:getitem_7 meta: {'nn_module_stack': OrderedDict({'self_attn': ('self_attn', <class 'nandmachine.frontend.network.qwen3.Qwen3Attention'>), 'self_attn.rotary_emb': ('self_attn.rotary_emb', <class 'nandmachine.frontend.network.torch_kernels.RotaryEmbedding'>)}), 'val': FakeTensor(..., size=(16, 32, 64)), 'output_shapes': (16, 32, 64), 'input_shapes': [[(16, 32, 64), (16, 32, 64)]]}\n",
      "node name:mul meta: {'nn_module_stack': OrderedDict({'self_attn': ('self_attn', <class 'nandmachine.frontend.network.qwen3.Qwen3Attention'>), 'self_attn.rotary_emb': ('self_attn.rotary_emb', <class 'nandmachine.frontend.network.torch_kernels.RotaryEmbedding'>)}), 'val': FakeTensor(..., size=(16, 32, 64)), 'output_shapes': (16, 32, 64), 'input_shapes': [(16, 32, 64), (16, 1, 64)]}\n",
      "node name:mul_1 meta: {'nn_module_stack': OrderedDict({'self_attn': ('self_attn', <class 'nandmachine.frontend.network.qwen3.Qwen3Attention'>), 'self_attn.rotary_emb': ('self_attn.rotary_emb', <class 'nandmachine.frontend.network.torch_kernels.RotaryEmbedding'>)}), 'val': FakeTensor(..., size=(16, 32, 64)), 'output_shapes': (16, 32, 64), 'input_shapes': [(16, 32, 64), (16, 1, 64)]}\n",
      "node name:sub meta: {'nn_module_stack': OrderedDict({'self_attn': ('self_attn', <class 'nandmachine.frontend.network.qwen3.Qwen3Attention'>), 'self_attn.rotary_emb': ('self_attn.rotary_emb', <class 'nandmachine.frontend.network.torch_kernels.RotaryEmbedding'>)}), 'val': FakeTensor(..., size=(16, 32, 64)), 'output_shapes': (16, 32, 64), 'input_shapes': [(16, 32, 64), (16, 32, 64)]}\n",
      "node name:mul_2 meta: {'nn_module_stack': OrderedDict({'self_attn': ('self_attn', <class 'nandmachine.frontend.network.qwen3.Qwen3Attention'>), 'self_attn.rotary_emb': ('self_attn.rotary_emb', <class 'nandmachine.frontend.network.torch_kernels.RotaryEmbedding'>)}), 'val': FakeTensor(..., size=(16, 32, 64)), 'output_shapes': (16, 32, 64), 'input_shapes': [(16, 32, 64), (16, 1, 64)]}\n",
      "node name:mul_3 meta: {'nn_module_stack': OrderedDict({'self_attn': ('self_attn', <class 'nandmachine.frontend.network.qwen3.Qwen3Attention'>), 'self_attn.rotary_emb': ('self_attn.rotary_emb', <class 'nandmachine.frontend.network.torch_kernels.RotaryEmbedding'>)}), 'val': FakeTensor(..., size=(16, 32, 64)), 'output_shapes': (16, 32, 64), 'input_shapes': [(16, 32, 64), (16, 1, 64)]}\n",
      "node name:add meta: {'nn_module_stack': OrderedDict({'self_attn': ('self_attn', <class 'nandmachine.frontend.network.qwen3.Qwen3Attention'>), 'self_attn.rotary_emb': ('self_attn.rotary_emb', <class 'nandmachine.frontend.network.torch_kernels.RotaryEmbedding'>)}), 'val': FakeTensor(..., size=(16, 32, 64)), 'output_shapes': (16, 32, 64), 'input_shapes': [(16, 32, 64), (16, 32, 64)]}\n",
      "node name:cat meta: {'nn_module_stack': OrderedDict({'self_attn': ('self_attn', <class 'nandmachine.frontend.network.qwen3.Qwen3Attention'>), 'self_attn.rotary_emb': ('self_attn.rotary_emb', <class 'nandmachine.frontend.network.torch_kernels.RotaryEmbedding'>)}), 'val': FakeTensor(..., size=(16, 32, 128)), 'output_shapes': (16, 32, 128), 'input_shapes': [[(16, 32, 64), (16, 32, 64)]]}\n",
      "node name:getattr_1 meta: {'nn_module_stack': OrderedDict({'self_attn': ('self_attn', <class 'nandmachine.frontend.network.qwen3.Qwen3Attention'>), 'self_attn.rotary_emb': ('self_attn.rotary_emb', <class 'nandmachine.frontend.network.torch_kernels.RotaryEmbedding'>)}), 'input_shapes': [(16, 32, 128)]}\n",
      "node name:to meta: {'nn_module_stack': OrderedDict({'self_attn': ('self_attn', <class 'nandmachine.frontend.network.qwen3.Qwen3Attention'>), 'self_attn.rotary_emb': ('self_attn.rotary_emb', <class 'nandmachine.frontend.network.torch_kernels.RotaryEmbedding'>)}), 'val': FakeTensor(..., size=(16, 32, 128)), 'output_shapes': (16, 32, 128), 'input_shapes': [(16, 32, 128)]}\n",
      "node name:float_2 meta: {'nn_module_stack': OrderedDict({'self_attn': ('self_attn', <class 'nandmachine.frontend.network.qwen3.Qwen3Attention'>), 'self_attn.rotary_emb': ('self_attn.rotary_emb', <class 'nandmachine.frontend.network.torch_kernels.RotaryEmbedding'>)}), 'val': FakeTensor(..., size=(16, 8, 128)), 'output_shapes': (16, 8, 128), 'input_shapes': [(16, 8, 128)]}\n",
      "node name:chunk_2 meta: {'nn_module_stack': OrderedDict({'self_attn': ('self_attn', <class 'nandmachine.frontend.network.qwen3.Qwen3Attention'>), 'self_attn.rotary_emb': ('self_attn.rotary_emb', <class 'nandmachine.frontend.network.torch_kernels.RotaryEmbedding'>)}), 'val': (FakeTensor(..., size=(16, 8, 64)), FakeTensor(..., size=(16, 8, 64))), 'output_shapes': [(16, 8, 64), (16, 8, 64)], 'input_shapes': [(16, 8, 128)]}\n",
      "node name:getitem_8 meta: {'nn_module_stack': OrderedDict({'self_attn': ('self_attn', <class 'nandmachine.frontend.network.qwen3.Qwen3Attention'>), 'self_attn.rotary_emb': ('self_attn.rotary_emb', <class 'nandmachine.frontend.network.torch_kernels.RotaryEmbedding'>)}), 'val': FakeTensor(..., size=(16, 8, 64)), 'output_shapes': (16, 8, 64), 'input_shapes': [[(16, 8, 64), (16, 8, 64)]]}\n",
      "node name:getitem_9 meta: {'nn_module_stack': OrderedDict({'self_attn': ('self_attn', <class 'nandmachine.frontend.network.qwen3.Qwen3Attention'>), 'self_attn.rotary_emb': ('self_attn.rotary_emb', <class 'nandmachine.frontend.network.torch_kernels.RotaryEmbedding'>)}), 'val': FakeTensor(..., size=(16, 8, 64)), 'output_shapes': (16, 8, 64), 'input_shapes': [[(16, 8, 64), (16, 8, 64)]]}\n",
      "node name:mul_4 meta: {'nn_module_stack': OrderedDict({'self_attn': ('self_attn', <class 'nandmachine.frontend.network.qwen3.Qwen3Attention'>), 'self_attn.rotary_emb': ('self_attn.rotary_emb', <class 'nandmachine.frontend.network.torch_kernels.RotaryEmbedding'>)}), 'val': FakeTensor(..., size=(16, 8, 64)), 'output_shapes': (16, 8, 64), 'input_shapes': [(16, 8, 64), (16, 1, 64)]}\n",
      "node name:mul_5 meta: {'nn_module_stack': OrderedDict({'self_attn': ('self_attn', <class 'nandmachine.frontend.network.qwen3.Qwen3Attention'>), 'self_attn.rotary_emb': ('self_attn.rotary_emb', <class 'nandmachine.frontend.network.torch_kernels.RotaryEmbedding'>)}), 'val': FakeTensor(..., size=(16, 8, 64)), 'output_shapes': (16, 8, 64), 'input_shapes': [(16, 8, 64), (16, 1, 64)]}\n",
      "node name:sub_1 meta: {'nn_module_stack': OrderedDict({'self_attn': ('self_attn', <class 'nandmachine.frontend.network.qwen3.Qwen3Attention'>), 'self_attn.rotary_emb': ('self_attn.rotary_emb', <class 'nandmachine.frontend.network.torch_kernels.RotaryEmbedding'>)}), 'val': FakeTensor(..., size=(16, 8, 64)), 'output_shapes': (16, 8, 64), 'input_shapes': [(16, 8, 64), (16, 8, 64)]}\n",
      "node name:mul_6 meta: {'nn_module_stack': OrderedDict({'self_attn': ('self_attn', <class 'nandmachine.frontend.network.qwen3.Qwen3Attention'>), 'self_attn.rotary_emb': ('self_attn.rotary_emb', <class 'nandmachine.frontend.network.torch_kernels.RotaryEmbedding'>)}), 'val': FakeTensor(..., size=(16, 8, 64)), 'output_shapes': (16, 8, 64), 'input_shapes': [(16, 8, 64), (16, 1, 64)]}\n",
      "node name:mul_7 meta: {'nn_module_stack': OrderedDict({'self_attn': ('self_attn', <class 'nandmachine.frontend.network.qwen3.Qwen3Attention'>), 'self_attn.rotary_emb': ('self_attn.rotary_emb', <class 'nandmachine.frontend.network.torch_kernels.RotaryEmbedding'>)}), 'val': FakeTensor(..., size=(16, 8, 64)), 'output_shapes': (16, 8, 64), 'input_shapes': [(16, 8, 64), (16, 1, 64)]}\n",
      "node name:add_1 meta: {'nn_module_stack': OrderedDict({'self_attn': ('self_attn', <class 'nandmachine.frontend.network.qwen3.Qwen3Attention'>), 'self_attn.rotary_emb': ('self_attn.rotary_emb', <class 'nandmachine.frontend.network.torch_kernels.RotaryEmbedding'>)}), 'val': FakeTensor(..., size=(16, 8, 64)), 'output_shapes': (16, 8, 64), 'input_shapes': [(16, 8, 64), (16, 8, 64)]}\n",
      "node name:cat_1 meta: {'nn_module_stack': OrderedDict({'self_attn': ('self_attn', <class 'nandmachine.frontend.network.qwen3.Qwen3Attention'>), 'self_attn.rotary_emb': ('self_attn.rotary_emb', <class 'nandmachine.frontend.network.torch_kernels.RotaryEmbedding'>)}), 'val': FakeTensor(..., size=(16, 8, 128)), 'output_shapes': (16, 8, 128), 'input_shapes': [[(16, 8, 64), (16, 8, 64)]]}\n",
      "node name:getattr_2 meta: {'nn_module_stack': OrderedDict({'self_attn': ('self_attn', <class 'nandmachine.frontend.network.qwen3.Qwen3Attention'>), 'self_attn.rotary_emb': ('self_attn.rotary_emb', <class 'nandmachine.frontend.network.torch_kernels.RotaryEmbedding'>)}), 'input_shapes': [(16, 8, 128)]}\n",
      "node name:to_1 meta: {'nn_module_stack': OrderedDict({'self_attn': ('self_attn', <class 'nandmachine.frontend.network.qwen3.Qwen3Attention'>), 'self_attn.rotary_emb': ('self_attn.rotary_emb', <class 'nandmachine.frontend.network.torch_kernels.RotaryEmbedding'>)}), 'val': FakeTensor(..., size=(16, 8, 128)), 'output_shapes': (16, 8, 128), 'input_shapes': [(16, 8, 128)]}\n",
      "node name:add_2 meta: {'nn_module_stack': OrderedDict({'self_attn': ('self_attn', <class 'nandmachine.frontend.network.qwen3.Qwen3Attention'>), 'self_attn.attn': ('self_attn.attn', <class 'nandmachine.frontend.network.torch_kernels.Attention'>)}), 'val': FakeTensor(..., size=(16, 32, 128)), 'output_shapes': (16, 32, 128), 'input_shapes': [(16, 32, 128)]}\n",
      "node name:flatten meta: {'nn_module_stack': OrderedDict({'self_attn': ('self_attn', <class 'nandmachine.frontend.network.qwen3.Qwen3Attention'>)}), 'val': FakeTensor(..., size=(16, 4096)), 'output_shapes': (16, 4096), 'input_shapes': [(16, 32, 128)]}\n",
      "node name:self_attn_o_proj_weight meta: {'nn_module_stack': OrderedDict({'self_attn': ('self_attn', <class 'nandmachine.frontend.network.qwen3.Qwen3Attention'>), 'self_attn.o_proj': ('self_attn.o_proj', <class 'nandmachine.frontend.network.torch_kernels.RowParallelLinear'>)}), 'val': FakeTensor(..., size=(4096, 4096)), 'output_shapes': (4096, 4096)}\n",
      "node name:linear_1 meta: {'nn_module_stack': OrderedDict({'self_attn': ('self_attn', <class 'nandmachine.frontend.network.qwen3.Qwen3Attention'>), 'self_attn.o_proj': ('self_attn.o_proj', <class 'nandmachine.frontend.network.torch_kernels.RowParallelLinear'>)}), 'val': FakeTensor(..., size=(16, 4096)), 'output_shapes': (16, 4096), 'input_shapes': [(16, 4096), (4096, 4096)]}\n",
      "node name:add_3 meta: {'val': FakeTensor(..., size=(16, 16, 4096)), 'output_shapes': (16, 16, 4096), 'input_shapes': [(16, 4096), (16, 1, 4096)]}\n",
      "node name:mlp_gate_up_proj_weight meta: {'nn_module_stack': OrderedDict({'mlp': ('mlp', <class 'nandmachine.frontend.network.qwen3.Qwen3MLP'>), 'mlp.gate_up_proj': ('mlp.gate_up_proj', <class 'nandmachine.frontend.network.torch_kernels.MergedColumnParallelLinear'>)}), 'val': FakeTensor(..., size=(24576, 4096)), 'output_shapes': (24576, 4096)}\n",
      "node name:linear_2 meta: {'nn_module_stack': OrderedDict({'mlp': ('mlp', <class 'nandmachine.frontend.network.qwen3.Qwen3MLP'>), 'mlp.gate_up_proj': ('mlp.gate_up_proj', <class 'nandmachine.frontend.network.torch_kernels.MergedColumnParallelLinear'>)}), 'val': FakeTensor(..., size=(16, 16, 24576)), 'output_shapes': (16, 16, 24576), 'input_shapes': [(16, 16, 4096), (24576, 4096)]}\n",
      "node name:chunk_3 meta: {'nn_module_stack': OrderedDict({'mlp': ('mlp', <class 'nandmachine.frontend.network.qwen3.Qwen3MLP'>), 'mlp.act_fn': ('mlp.act_fn', <class 'nandmachine.frontend.network.torch_kernels.SiluAndMul'>)}), 'val': (FakeTensor(..., size=(16, 16, 12288)), FakeTensor(..., size=(16, 16, 12288))), 'output_shapes': [(16, 16, 12288), (16, 16, 12288)], 'input_shapes': [(16, 16, 24576)]}\n",
      "node name:getitem_10 meta: {'nn_module_stack': OrderedDict({'mlp': ('mlp', <class 'nandmachine.frontend.network.qwen3.Qwen3MLP'>), 'mlp.act_fn': ('mlp.act_fn', <class 'nandmachine.frontend.network.torch_kernels.SiluAndMul'>)}), 'val': FakeTensor(..., size=(16, 16, 12288)), 'output_shapes': (16, 16, 12288), 'input_shapes': [[(16, 16, 12288), (16, 16, 12288)]]}\n",
      "node name:getitem_11 meta: {'nn_module_stack': OrderedDict({'mlp': ('mlp', <class 'nandmachine.frontend.network.qwen3.Qwen3MLP'>), 'mlp.act_fn': ('mlp.act_fn', <class 'nandmachine.frontend.network.torch_kernels.SiluAndMul'>)}), 'val': FakeTensor(..., size=(16, 16, 12288)), 'output_shapes': (16, 16, 12288), 'input_shapes': [[(16, 16, 12288), (16, 16, 12288)]]}\n",
      "node name:silu meta: {'nn_module_stack': OrderedDict({'mlp': ('mlp', <class 'nandmachine.frontend.network.qwen3.Qwen3MLP'>), 'mlp.act_fn': ('mlp.act_fn', <class 'nandmachine.frontend.network.torch_kernels.SiluAndMul'>)}), 'val': FakeTensor(..., size=(16, 16, 12288)), 'output_shapes': (16, 16, 12288), 'input_shapes': [(16, 16, 12288)]}\n",
      "node name:mul_8 meta: {'nn_module_stack': OrderedDict({'mlp': ('mlp', <class 'nandmachine.frontend.network.qwen3.Qwen3MLP'>), 'mlp.act_fn': ('mlp.act_fn', <class 'nandmachine.frontend.network.torch_kernels.SiluAndMul'>)}), 'val': FakeTensor(..., size=(16, 16, 12288)), 'output_shapes': (16, 16, 12288), 'input_shapes': [(16, 16, 12288), (16, 16, 12288)]}\n",
      "node name:mlp_down_proj_weight meta: {'nn_module_stack': OrderedDict({'mlp': ('mlp', <class 'nandmachine.frontend.network.qwen3.Qwen3MLP'>), 'mlp.down_proj': ('mlp.down_proj', <class 'nandmachine.frontend.network.torch_kernels.RowParallelLinear'>)}), 'val': FakeTensor(..., size=(4096, 12288)), 'output_shapes': (4096, 12288)}\n",
      "node name:linear_3 meta: {'nn_module_stack': OrderedDict({'mlp': ('mlp', <class 'nandmachine.frontend.network.qwen3.Qwen3MLP'>), 'mlp.down_proj': ('mlp.down_proj', <class 'nandmachine.frontend.network.torch_kernels.RowParallelLinear'>)}), 'val': FakeTensor(..., size=(16, 16, 4096)), 'output_shapes': (16, 16, 4096), 'input_shapes': [(16, 16, 12288), (4096, 12288)]}\n",
      "node name:add_4 meta: {'val': FakeTensor(..., size=(16, 16, 4096)), 'output_shapes': (16, 16, 4096), 'input_shapes': [(16, 16, 4096), (16, 16, 4096)]}\n"
     ]
    }
   ],
   "source": [
    "for node in graph.nodes:\n",
    "    print(f'node name:{node.name} meta: {node.meta}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4699574",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6194d2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda: machine",
   "language": "python",
   "name": "machine"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
